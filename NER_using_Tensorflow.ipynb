{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER using Tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1d2bIEEKOeyBSCaOXtjEooo31eubhE9Kj",
      "authorship_tag": "ABX9TyPXVJf82t4xuJOzoTRKoxS/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zenithexpo/Major-Project/blob/master/NER_using_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ5Hm_reN8kG",
        "outputId": "2a1c81fb-3df7-4cf8-8464-b35ca7d2e313"
      },
      "source": [
        "cd drive/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7VOjjZbOIaS",
        "outputId": "3cf80549-ad49-4277-cdf5-b16190df22cb"
      },
      "source": [
        "cd MyDrive/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Dmo-91rOIeX",
        "outputId": "84d10873-f9f3-48de-89f1-8c9d3d1ce627"
      },
      "source": [
        "cd ner/"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/ner\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHYPPZ_oOIlx",
        "outputId": "7884d686-de77-48a4-a1b8-71991c33b239"
      },
      "source": [
        "cd ner_tensorflow/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/ner/ner_tensorflow\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXdB6rQhOfqJ",
        "outputId": "c0485753-cab2-40bc-a886-9fa3a04a6248"
      },
      "source": [
        "!mkdir data\r\n",
        "!mkdir models"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "mkdir: cannot create directory ‘models’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw4r6InzOfyw",
        "outputId": "d8bc6874-954b-4956-93be-7c72399ee7a7"
      },
      "source": [
        "!curl https://groups.csail.mit.edu/sls/downloads/movie/engtest.bio -o data/test.txt\r\n",
        "!curl https://groups.csail.mit.edu/sls/downloads/movie/engtrain.bio -o data/train.txt"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  246k  100  246k    0     0   249k      0 --:--:-- --:--:-- --:--:--  249k\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  989k  100  989k    0     0  1120k      0 --:--:-- --:--:-- --:--:-- 1119k\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 824
        },
        "id": "NK59ZTtihEL-",
        "outputId": "b6429f5d-351d-4c27-9583-cfae92e028b5"
      },
      "source": [
        "!pip install tensorflow==1.13.1"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.13.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/29/6b4f1e02417c3a1ccc85380f093556ffd0b35dc354078074c5195c8447f2/tensorflow-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (92.6MB)\n",
            "\u001b[K     |████████████████████████████████| 92.6MB 83kB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (3.12.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.32.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.36.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.3.3)\n",
            "Collecting tensorflow-estimator<1.14.0rc0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/48/13f49fc3fa0fdf916aa1419013bb8f2ad09674c275b4046d5ee669a46873/tensorflow_estimator-1.13.0-py2.py3-none-any.whl (367kB)\n",
            "\u001b[K     |████████████████████████████████| 368kB 41.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.15.0)\n",
            "Collecting tensorboard<1.14.0,>=1.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/39/bdd75b08a6fba41f098b6cb091b9e8c7a80e1b4d679a581a0ccd17b10373/tensorboard-1.13.1-py3-none-any.whl (3.2MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 42.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.1.2)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (1.19.5)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.13.1) (0.10.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.6->tensorflow==1.13.1) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow==1.13.1) (54.0.0)\n",
            "Collecting mock>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/03/b7e605db4a57c0f6fba744b11ef3ddf4ddebcada35022927a2b5fc623fdf/mock-4.0.3-py3-none-any.whl\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.3.4)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.14.0,>=1.13.0->tensorflow==1.13.1) (3.4.0)\n",
            "Installing collected packages: keras-applications, mock, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 2.4.0\n",
            "    Uninstalling tensorflow-estimator-2.4.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.4.0\n",
            "  Found existing installation: tensorboard 2.4.1\n",
            "    Uninstalling tensorboard-2.4.1:\n",
            "      Successfully uninstalled tensorboard-2.4.1\n",
            "  Found existing installation: tensorflow 2.4.1\n",
            "    Uninstalling tensorflow-2.4.1:\n",
            "      Successfully uninstalled tensorflow-2.4.1\n",
            "Successfully installed keras-applications-1.0.8 mock-4.0.3 tensorboard-1.13.1 tensorflow-1.13.1 tensorflow-estimator-1.13.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XHJqlUIhmbh",
        "outputId": "1cf31478-ea13-4c6c-d936-1acdaa6481de"
      },
      "source": [
        "!pip install keras==2.2.4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 4.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.19.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.13)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLkz7Ev8Mr3a",
        "outputId": "186226ce-3335-4b5b-98e2-02e2b7010e3b"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from functools import partial\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "from keras.utils import to_categorical\r\n",
        "import pickle\r\n",
        "\r\n",
        "params = {\r\n",
        "    'dim' : 300,# dimension of embeddings\r\n",
        "    'maximum_steps' : 1000,# number of training steps\r\n",
        "    'lstm_size' : 150,# dimension of LSTM\r\n",
        "    'batch_size' : 25,# batch size\r\n",
        "    'max_words' : 10000,# maximum number of words to embed\r\n",
        "    'padding_size' : 20,# maximum sentence size\r\n",
        "    'num_classes' : 14,# number of unique classes\r\n",
        "    'save_dir' : 'models/'# directory to save hash tables, model weights, etc.\r\n",
        "}\r\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4s1y4B8NNVl"
      },
      "source": [
        "def save_obj(directory, obj, name):\r\n",
        "    '''Helper function using pickle to save and load objects'''\r\n",
        "    with open(directory + name + '.pkl', 'wb+') as f:\r\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\r\n",
        "\r\n",
        "def load_obj(directory, name):\r\n",
        "    '''Helper function using pickle to save and load objects'''\r\n",
        "    with open(directory + name + \".pkl\", \"rb\") as f:\r\n",
        "        return pickle.load(f)\r\n",
        "   \r\n",
        "def load_data(file = \"data/train.txt\"):\r\n",
        "    '''Helper function to load and transform inputs and labels\r\n",
        "    included as a separate function due to NER-specific evaluation needs:\r\n",
        "        tensorflow does not have multi-class precision/accuracy as a metric\r\n",
        "        so data_y is needed to manually calculate evaluations'''\r\n",
        "    file = open(file, 'r')\r\n",
        "    sentence, labels = [], []\r\n",
        "    data_x, data_y = [], []\r\n",
        "    for line in file:\r\n",
        "        line = line.strip(\"\\n\").split(\"\\t\")\r\n",
        "       \r\n",
        "        # lines with len > 1 are words\r\n",
        "        if len(line) > 1:\r\n",
        "            sentence.append(line[1])\r\n",
        "            labels.append(line[0][2:]) if len(line[0]) > 1 else labels.append(line[0])\r\n",
        "       \r\n",
        "        # lins with len == 1 are sentence breaks\r\n",
        "        if len(line) == 1:\r\n",
        "            data_x.append(' '.join(sentence))\r\n",
        "            data_y.append(labels)\r\n",
        "            sentence, labels = [], []\r\n",
        "    return data_x, data_y\r\n",
        "\r\n",
        "def make_tokenizer(file = \"data/train.txt\", params = params):\r\n",
        "    ''' In order for one hot encoding of words and labels to work,\r\n",
        "    every word and label has to be seen at least once to make a hashing table.\r\n",
        "    This function outputs hash tables for the words and the labels\r\n",
        "    that can be used to one-hot-encode them in the generator\r\n",
        "    '''\r\n",
        "    # Load parameters and data\r\n",
        "    max_words = params['max_words']\r\n",
        "    padding_size = params['padding_size']\r\n",
        "    save_dir = params['save_dir']\r\n",
        "    data_x, data_y = load_data(file)\r\n",
        "           \r\n",
        "    # Use the Keras tokenizer API to generate hashing table for data_x\r\n",
        "    tokenizer = Tokenizer(num_words = max_words)\r\n",
        "   \r\n",
        "    tokenizer.fit_on_texts(data_x)\r\n",
        "    word_index = tokenizer.word_index\r\n",
        "   \r\n",
        "    # Flatten data_y and create hashing table using set logic\r\n",
        "    data_y_flattened = [item for sublist in data_y for item in sublist]\r\n",
        "    data_x_flattened = [item for sublist in data_x for item in sublist]\r\n",
        "   \r\n",
        "    labels_index = dict([(y, x + 1) for x, y in enumerate(sorted(set(data_y_flattened)))])\r\n",
        "    labels = []\r\n",
        "    for item in data_y:\r\n",
        "        labels.append([labels_index.get(i) for i in item])\r\n",
        "    labels_lookup = {v : k for k, v in labels_index.items()} # reverse dictionary for lookup\r\n",
        "    # save hash tables to disk for model serving\r\n",
        "    for item, name in zip([word_index, labels_index, labels_lookup],\r\n",
        "                          [\"word_index\", \"labels_index\", \"labels_lookup\"]):\r\n",
        "        save_obj(save_dir, item, name)\r\n",
        "    return word_index, labels_index, labels_lookup\r\n",
        "\r\n",
        "word_index, labels_index, labels_lookup = make_tokenizer()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCpRXujWOrqS",
        "outputId": "40ab05a5-9508-4372-fc73-9bfff12128d8"
      },
      "source": [
        "labels_index\r\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ACTOR': 1,\n",
              " 'CHARACTER': 2,\n",
              " 'DIRECTOR': 3,\n",
              " 'GENRE': 4,\n",
              " 'O': 5,\n",
              " 'PLOT': 6,\n",
              " 'RATING': 7,\n",
              " 'RATINGS_AVERAGE': 8,\n",
              " 'REVIEW': 9,\n",
              " 'SONG': 10,\n",
              " 'TITLE': 11,\n",
              " 'TRAILER': 12,\n",
              " 'YEAR': 13}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gx3uuRhXOuUv"
      },
      "source": [
        "def generate_batches(file = \"data/train.txt\", params = params, train = True):\r\n",
        "    ''' Generate minibatch with dimensions:\r\n",
        "    batch_x : (batch_size, max_len)\r\n",
        "    lengths : (batch_size,)\r\n",
        "    batch_y : (batch_size, num_classes)\r\n",
        "   \r\n",
        "    file : path to .txt containing training data in BIO format\r\n",
        "    '''\r\n",
        "   \r\n",
        "    batch_size = params['batch_size']\r\n",
        "    max_len = params['padding_size']\r\n",
        "    save_dir = params['save_dir']\r\n",
        "   \r\n",
        "    # load hash tables for tokenization\r\n",
        "    for item, name in zip([word_index, labels_index, labels_lookup],\r\n",
        "                          [\"word_index\", \"labels_index\", \"labels_lookup\"]):\r\n",
        "        item = load_obj(save_dir, name)\r\n",
        "   \r\n",
        "    while True:\r\n",
        "        with open(file, 'r') as f:\r\n",
        "            batch_x, lengths, batch_y = [], [], []\r\n",
        "            words, labels = [], []\r\n",
        "            for line in f:\r\n",
        "                line = line.strip(\"\\n\").split(\"\\t\")\r\n",
        "                # lines with len > 1 are words\r\n",
        "                if len(line) > 1:\r\n",
        "                    labels.append(line[0][2:]) if len(line[0]) > 1 else labels.append(line[0])\r\n",
        "                    words.append(line[1])\r\n",
        "\r\n",
        "                # lines with len == 1 are breaks between sentences\r\n",
        "                if len(line) == 1:\r\n",
        "                    words = [word_index.get(x) if x in word_index.keys() else 0 for x in words]\r\n",
        "                    labels = [labels_index.get(y) for y in labels]\r\n",
        "                    batch_x.append(words)\r\n",
        "                    batch_y.append(labels)\r\n",
        "                    lengths.append(min(len(words), max_len))\r\n",
        "                    words, labels = [], []\r\n",
        "\r\n",
        "                if len(batch_x) == batch_size:\r\n",
        "                    batch_x = pad_sequences(batch_x, maxlen = max_len, value = 0, padding = \"post\")\r\n",
        "                    batch_y = pad_sequences(batch_y, maxlen = max_len, value = 0, padding = \"post\")\r\n",
        "                    yield (batch_x, lengths), batch_y\r\n",
        "                    batch_x, lengths, batch_y = [], [], []\r\n",
        "            if train == False:\r\n",
        "                break"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1HGJYx2Oy5j"
      },
      "source": [
        "# For model training, we need an input function that will feed a tf.Dataset\r\n",
        "def input_fn(file, params = None, train = True):\r\n",
        "    params = params if params is not None else {}\r\n",
        "    shapes = (([None, None], [None]), [None, None]) # batch_x, lengths, batch_y shapes\r\n",
        "    types = ((tf.int32, tf.int32), tf.int32)        # batch_x, lengths, batch_y data types\r\n",
        "   \r\n",
        "    generator = partial(generate_batches, file, train = train)\r\n",
        "    dataset = tf.data.Dataset.from_generator(generator, types, shapes)\r\n",
        "    return dataset\r\n",
        "\r\n",
        "# For model serving, we need a serving function that will feed tf.placeholders\r\n",
        "def serving_input_fn():\r\n",
        "    words = tf.placeholder(dtype=tf.int32, shape=[None, None], name='words')\r\n",
        "    length = tf.placeholder(dtype=tf.int32, shape=[None], name='length')\r\n",
        "    receiver_tensors = {'words': words, 'length': length}\r\n",
        "    features = {'words': words, 'length': length}\r\n",
        "    return tf.estimator.export.ServingInputReceiver(features, receiver_tensors)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp5r_OUOO2GW"
      },
      "source": [
        "def model_fn(features, labels, mode, params = params):\r\n",
        "    # import the data and unpack the features\r\n",
        "    # serving input_fn returns a dict, convert to multivalue obj\r\n",
        "    if isinstance(features, dict):\r\n",
        "        features = features['words'], features['length']\r\n",
        "   \r\n",
        "    words, length = features\r\n",
        "   \r\n",
        "    # Embedding\r\n",
        "    embedding = tf.Variable(tf.random_normal([params['max_words'], params['dim']]))\r\n",
        "    embedding_lookup_for_x = tf.nn.embedding_lookup(embedding, words)\r\n",
        "   \r\n",
        "    # LSTM\r\n",
        "    lstm_cell_fw = tf.nn.rnn_cell.BasicLSTMCell(params['lstm_size'], state_is_tuple = True)\r\n",
        "    lstm_cell_bw = tf.nn.rnn_cell.BasicLSTMCell(params['lstm_size'], state_is_tuple = True)\r\n",
        "    states, final_state = tf.nn.bidirectional_dynamic_rnn(\r\n",
        "                                        cell_fw = lstm_cell_fw,\r\n",
        "                                        cell_bw = lstm_cell_bw,\r\n",
        "                                        inputs = embedding_lookup_for_x,\r\n",
        "                                        dtype = tf.float32,\r\n",
        "                                        time_major = False,\r\n",
        "                                        sequence_length = length)\r\n",
        "    lstm_out = tf.concat([states[0], states[1]], axis = 2)\r\n",
        "       \r\n",
        "    # Conditional random fields\r\n",
        "    logits = tf.layers.dense(lstm_out, params['num_classes'])\r\n",
        "    crf_params = tf.get_variable(\"crf\", [params['num_classes'], params['num_classes']],\r\n",
        "                                 dtype=tf.float32)\r\n",
        "    pred_ids, _ = tf.contrib.crf.crf_decode(logits, crf_params, length)\r\n",
        "    training = (mode == tf.estimator.ModeKeys.TRAIN)\r\n",
        "   \r\n",
        "    # Prediction\r\n",
        "    if mode == tf.estimator.ModeKeys.PREDICT:\r\n",
        "        predictions = {\r\n",
        "            'pred_ids': pred_ids,\r\n",
        "            'tags': words,\r\n",
        "            'length' : length,\r\n",
        "        }\r\n",
        "        export_outputs = {\r\n",
        "          'prediction': tf.estimator.export.PredictOutput(predictions)\r\n",
        "      }\r\n",
        "       \r\n",
        "        return tf.estimator.EstimatorSpec(mode, predictions=predictions,\r\n",
        "                                          export_outputs=export_outputs)\r\n",
        "   \r\n",
        "    # Loss functions and optimizers\r\n",
        "    log_likelihood, _ = tf.contrib.crf.crf_log_likelihood(\r\n",
        "        logits, labels, length, crf_params)\r\n",
        "   \r\n",
        "    loss = tf.reduce_mean(-log_likelihood)\r\n",
        "    train_op = tf.train.AdamOptimizer().minimize(\r\n",
        "        loss, global_step = tf.train.get_or_create_global_step())\r\n",
        "       \r\n",
        "    # Training\r\n",
        "    if mode == tf.estimator.ModeKeys.TRAIN:\r\n",
        "        return tf.estimator.EstimatorSpec(mode = mode,\r\n",
        "                                           loss = loss,\r\n",
        "                                           train_op = train_op)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PTyXXcASO5FO",
        "outputId": "000e920c-7b69-48c4-9747-f3a66da06ffa"
      },
      "source": [
        "# Spin up the estimator\r\n",
        "config = tf.estimator.RunConfig()\r\n",
        "estimator = tf.estimator.Estimator(model_fn, 'models/model', config, params)\r\n",
        "\r\n",
        "# Create train spec\r\n",
        "train_input_fn = partial(input_fn, \"data/train.txt\", params = params)\r\n",
        "train_spec = tf.estimator.TrainSpec(train_input_fn)\r\n",
        "\r\n",
        "# Create evaluation spec\r\n",
        "eval_input_fn = partial(input_fn, \"data/test.txt\", params = params, train = False)\r\n",
        "eval_spec = tf.estimator.EvalSpec(eval_input_fn)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using config: {'_model_dir': 'models/model', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe82cf2d2d0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WubCasn0PBXc"
      },
      "source": [
        "import time"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbkU5jbDO7yS",
        "outputId": "4c63a04b-809d-4cd2-fe08-80cdaccc223d"
      },
      "source": [
        "ts = time.time()\r\n",
        "estimator.train(input_fn = train_input_fn, max_steps = 1000)\r\n",
        "te = time.time()\r\n",
        "print(\"Completed in {} seconds\".format(int(te - ts)))\r\n",
        "estimator.export_savedmodel('models/saved_model/', serving_input_fn)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/data/ops/dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "tf.py_func is deprecated in TF V2. Instead, use\n",
            "    tf.py_function, which takes a python function which manipulates tf eager\n",
            "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
            "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
            "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
            "    being differentiable using a gradient tape.\n",
            "    \n",
            "INFO:tensorflow:Calling model_fn.\n",
            "WARNING:tensorflow:From <ipython-input-12-eb29994ffb56>:14: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-12-eb29994ffb56>:22: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/rnn.py:443: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/rnn.py:626: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "WARNING:tensorflow:From <ipython-input-12-eb29994ffb56>:26: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.dense instead.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_impl.py:110: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into models/model/model.ckpt.\n",
            "INFO:tensorflow:loss = 25.802078, step = 1\n",
            "INFO:tensorflow:global_step/sec: 7.91578\n",
            "INFO:tensorflow:loss = 5.56465, step = 101 (12.636 sec)\n",
            "INFO:tensorflow:global_step/sec: 7.45503\n",
            "INFO:tensorflow:loss = 2.5894566, step = 201 (13.413 sec)\n",
            "INFO:tensorflow:global_step/sec: 6.74372\n",
            "INFO:tensorflow:loss = 1.5749393, step = 301 (14.828 sec)\n",
            "INFO:tensorflow:global_step/sec: 7.42633\n",
            "INFO:tensorflow:loss = 3.9944944, step = 401 (13.467 sec)\n",
            "INFO:tensorflow:global_step/sec: 8.58904\n",
            "INFO:tensorflow:loss = 6.255591, step = 501 (11.642 sec)\n",
            "INFO:tensorflow:global_step/sec: 7.37029\n",
            "INFO:tensorflow:loss = 1.154845, step = 601 (13.569 sec)\n",
            "INFO:tensorflow:global_step/sec: 6.81161\n",
            "INFO:tensorflow:loss = 0.4085395, step = 701 (14.680 sec)\n",
            "INFO:tensorflow:global_step/sec: 7.53114\n",
            "INFO:tensorflow:loss = 3.0346093, step = 801 (13.278 sec)\n",
            "INFO:tensorflow:global_step/sec: 8.53399\n",
            "INFO:tensorflow:loss = 1.8910985, step = 901 (11.719 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 1000 into models/model/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.34313813.\n",
            "Completed in 140 seconds\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['prediction', 'serving_default']\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
            "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from models/model/model.ckpt-1000\n",
            "INFO:tensorflow:Assets added to graph.\n",
            "INFO:tensorflow:No assets to write.\n",
            "INFO:tensorflow:SavedModel written to: models/saved_model/temp-b'1614950330'/saved_model.pb\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'models/saved_model/1614950330'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8K1piXLrIBn"
      },
      "source": [
        "import numpy as np\r\n",
        "import keras\r\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJQMn2VkumLg"
      },
      "source": [
        "class Recall(tf.keras.metrics.Recall):\r\n",
        "\r\n",
        "  def __init__(self, *, class_id, **kwargs):\r\n",
        "    super().__init__(**kwargs)\r\n",
        "    self.class_id= class_id\r\n",
        "\r\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\r\n",
        "    y_true = y_true[:, self.class_id]\r\n",
        "    y_pred = tf.cast(tf.equal(\r\n",
        "      tf.math.argmax(y_pred, axis=-1), self.class_id), dtype=tf.float32)\r\n",
        "    return super().update_state(y_true, y_pred, sample_weight)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "6gj6Ks6gO_WO",
        "outputId": "5798de20-f05a-4b80-92f5-6c221941f263"
      },
      "source": [
        "# Generate predictions\r\n",
        "predictions = estimator.predict(eval_input_fn)\r\n",
        "\r\n",
        "# Load hash tables and true labels\r\n",
        "labels_index = load_obj(params['save_dir'], \"labels_index\")\r\n",
        "_, true = load_data(\"data/test.txt\")\r\n",
        "\r\n",
        "# Specify which label_index is non-entity\r\n",
        "dummy_label = labels_index.get(\"O\")\r\n",
        "\r\n",
        "# Convert [[string, string], [string, string] ...] to [[int, int], [int, int]]\r\n",
        "# with hashing table for label indexes\r\n",
        "labels = []\r\n",
        "for row in true:\r\n",
        "    labels.append([labels_index.get(y) for y in row])\r\n",
        "   \r\n",
        "# Loop through preds, labels and calculate metrics\r\n",
        "precisions, recalls, f1s = [], [], []\r\n",
        "for pred, truee in zip(predictions, labels):\r\n",
        "    pred = pred['pred_ids'][:pred['length']] # undo pad_sequences\r\n",
        "    pred = [x for x in pred if x != dummy_label] # remove preds that aren't entities\r\n",
        "    truee = np.asarray([x for x in true if x != dummy_label])\r\n",
        "    print(truee,pred)\r\n",
        "    m = tf.keras.metrics.Recall(truee,pred)\r\n",
        "    recall = m.result().numpy()\r\n",
        "    # recall.update_state(truee,pred)    \r\n",
        "    # recall = tf.keras.metrics.Recall(truee, pred)\r\n",
        "    recalls.append(recall)\r\n",
        "    precision = calc_precision(truee, pred)\r\n",
        "    precisions.append(precision)\r\n",
        "    f1s.append(calc_f1(precision, recall))\r\n",
        "   \r\n",
        "print(\"Precision: {} \\nRecall: {} \\nF1-score: {}\".format(np.around(np.mean(precisions), 3),\r\n",
        "                                                         np.around(np.mean(recalls), 3),\r\n",
        "                                                         np.around(np.mean(f1s), 3)))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from models/model/model.ckpt-1000\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "[list(['O', 'O', 'O', 'O', 'GENRE', 'GENRE', 'O', 'YEAR', 'YEAR'])\n",
            " list(['O', 'O', 'O', 'O', 'O', 'PLOT', 'PLOT', 'PLOT'])\n",
            " list(['O', 'O', 'RATINGS_AVERAGE', 'RATINGS_AVERAGE', 'O', 'O', 'O', 'ACTOR', 'ACTOR'])\n",
            " ... list(['O', 'CHARACTER', 'CHARACTER', 'O'])\n",
            " list(['O', 'O', 'O', 'GENRE', 'O', 'O', 'CHARACTER', 'CHARACTER'])\n",
            " list(['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'CHARACTER', 'CHARACTER', 'CHARACTER'])] [4, 4]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  return array(a, dtype, copy=False, order=order)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-3f72271de0f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtruee\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdummy_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruee\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruee\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# recall.update_state(truee,pred)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, thresholds, name, dtype)\u001b[0m\n\u001b[1;32m   1243\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m     self.thresholds = _parse_init_thresholds(\n\u001b[0;32m-> 1245\u001b[0;31m         thresholds, default_threshold=0.5)\n\u001b[0m\u001b[1;32m   1246\u001b[0m     self.tp = self.add_weight(\n\u001b[1;32m   1247\u001b[0m         \u001b[0;34m'true_positives'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36m_parse_init_thresholds\u001b[0;34m(thresholds, default_threshold)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_parse_init_thresholds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresholds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_threshold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[0mthresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_threshold\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m   \u001b[0m_assert_thresholds_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mthresholds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36m_assert_thresholds_range\u001b[0;34m(thresholds)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_assert_thresholds_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m   \u001b[0minvalid_thresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minvalid_thresholds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     raise ValueError('Threshold values must be in [0, 1]. Invalid values: {}'\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_assert_thresholds_range\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthresholds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m   \u001b[0minvalid_thresholds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthresholds\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minvalid_thresholds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     raise ValueError('Threshold values must be in [0, 1]. Invalid values: {}'\n",
            "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'list' and 'int'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-A-nUCBrXk8",
        "outputId": "999a57a5-a2cd-4547-f92a-b6cdef4f26d2"
      },
      "source": [
        " from pathlib import Path\r\n",
        "from tensorflow.contrib import predictor\r\n",
        "\r\n",
        "LINE = 'did george clooney make a science fiction movie in the 1980s'\r\n",
        "\r\n",
        "\r\n",
        "def predict(line, export_dir = 'models/saved_model/', params = params):\r\n",
        "    # Load hash tables\r\n",
        "    word_index = load_obj(params['save_dir'], \"word_index\")\r\n",
        "    labels_lookup = load_obj(params['save_dir'], \"labels_lookup\")\r\n",
        "   \r\n",
        "    # Identify and load model weights\r\n",
        "    subdirs = [x for x in Path(export_dir).iterdir()\r\n",
        "                   if x.is_dir() and 'temp' not in str(x)]\r\n",
        "    latest_model = str(sorted(subdirs)[-1])\r\n",
        "    predict_fn = predictor.from_saved_model(latest_model)\r\n",
        "               \r\n",
        "    # Preprocess sentence input\r\n",
        "    line = line.strip().split()\r\n",
        "    vector = [word_index.get(x) if x in word_index.keys() else 0 for x in line] # tokenize\r\n",
        "    vector[len(vector):20] = [0] * (20 - len(vector)) # pad prediction\r\n",
        "       \r\n",
        "    # Calculate precision and transform for display\r\n",
        "    predictions = predict_fn({'words': [vector], 'length': [len(line)]})\r\n",
        "    tags = predictions.get('tags')\r\n",
        "    preds = predictions.get('pred_ids')\r\n",
        "    for tag, pred in zip(tags, preds):\r\n",
        "        tag = [word for word in tag if word != 0] # unpad\r\n",
        "        pred = pred[:len(tag)]\r\n",
        "        pred = [labels_lookup.get(num) for num in pred] #untokenize\r\n",
        "        print(line, \"\\n\", pred)\r\n",
        "   \r\n",
        "predict(LINE)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/contrib/predictor/saved_model_predictor.py:153: load (from tensorflow.python.saved_model.loader_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.loader.load or tf.compat.v1.saved_model.load. There will be a new function for importing SavedModels in Tensorflow 2.0.\n",
            "INFO:tensorflow:Restoring parameters from models/saved_model/1614950330/variables/variables\n",
            "['did', 'george', 'clooney', 'make', 'a', 'science', 'fiction', 'movie', 'in', 'the', '1980s'] \n",
            " ['O', 'ACTOR', 'ACTOR', 'O', 'O', 'GENRE', 'GENRE', 'O', 'O', 'O', 'YEAR']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UL0y4r_Q31kN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}